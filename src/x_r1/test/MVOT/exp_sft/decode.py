from transformers import AutoTokenizer

input_ids = [151644,   8948,    198,     32,  61637,   1948,   2657,    323,  21388,
            13,    576,   1196,  17064,    264,   3405,     11,    323,    279,
         21388,  67477,    432,  11568,  17847,   1156,  15482,    911,    279,
         32711,   1882,    304,    279,   3971,    323,   1221,   7424,    279,
         28439,   3059,  11568,  12433,  28439,   3059,   1265,    387,   3322,
           369,    279,  32711,   1882,    304,    279,   2937,   7354,  11568,
         32711,   1882,     11,   3255,     11,    323,   1590,   4226,    525,
         43810,   2878,  13708,    766,     29,    690,  26865,   8066,    366,
          8548,     29,    690,   8548,   8066,    323,    366,   9217,     29,
           690,   9217,     29,   9492,     11,  15576,     13, 151645,    198,
        151644,    872,    198,     32,  54272,   1459,    374,    264,   1459,
          6693,  13934,    525,   2176,  25780,     13,   2585,   1657,  54272,
          3501,    525,    389,    279,  18732,    476,   4766,    279,   5537,
         61115,    553,    400,     88,     28,     91,     87,  80050,    323,
           400,     88,  10829,     87,     61,     17,  41715,  37018,     90,
            18,     20,  15170,     19,  31716,     30, 151645,    198, 151644,
         77091,    198,   8304,    220,     16,    510,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   8304,    220,     22,    510,  13708,    766,     29,   1752,
           400,     87,     28,     17,      3,    510,     12,    400,     88,
            28,     91,     87,  87803,     17,  25046,     12,    400,     88,
         10829,     87,     61,     17,  41715,  37018,     90,     18,     20,
         15170,     19,  51185,     19,     13,     22,     20,  25046,     12,
          4440,    400,     88,      3,   2750,     25,    400,     17,     11,
           220,     18,     11,    220,     19,      3,    320,   5035,    220,
            18,   3501,  66233,  26865,    397,     27,   8548,     29,   1752,
           400,     87,     28,     17,  54876,   2697,    400,     88,      3,
          2750,     25,    400,     17,     11,    220,     18,     11,    220,
            19,      3,    320,     18,   3501,  66233,   8548,     29,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100]

# Initialize tokenizer
# You might need to replace "gpt2" with the specific tokenizer
# that was used to generate these input_ids if you know it.
tokenizer_name = "Qwen/Qwen2.5-1.5B-Instruct" 
try:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
except OSError:
    print(f"Tokenizer {tokenizer_name} not found. You might need to download it or specify a different one.")
    print("Attempting with 'EleutherAI/gpt-neo-125M' as an alternative.")
    tokenizer_name = "EleutherAI/gpt-neo-125M" # A smaller, common alternative
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    except OSError:
        print(f"Alternative tokenizer {tokenizer_name} also not found. Please ensure you have a valid tokenizer name and internet connection.")
        exit()


# Filter out any negative IDs, which are often used for padding or special tokens to ignore
filtered_input_ids = [id_ for id_ in input_ids if id_ >= 0]

# Decode the token IDs
decoded_text = tokenizer.decode(filtered_input_ids)

print(f"Decoded text using {tokenizer_name}:")
print(decoded_text)

# If your tokenizer has specific pad_token_id and it's not -100, 
# you might need to adjust the filtering or use skip_special_tokens=True in decode
# For example, if pad_token_id is 0 for a tokenizer:
# decoded_text_skip_special = tokenizer.decode(input_ids, skip_special_tokens=True)
# print("\nDecoded text skipping special tokens:")
# print(decoded_text_skip_special)